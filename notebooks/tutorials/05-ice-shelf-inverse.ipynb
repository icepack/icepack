{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse problems\n",
    "\n",
    "In this demo, we'll revisit the Larsen Ice Shelf.\n",
    "This time, we're going to estimate the fluidity coefficient $A$ in Glen's flow law\n",
    "\n",
    "$$\\dot\\varepsilon = A\\tau^3$$\n",
    "\n",
    "from observational data.\n",
    "In the previous demos, we've come up with some value of the fluidity coefficient and computed a velocity field by solving an elliptic partial differential equation.\n",
    "The fluidity coefficient is roughly a known function of the ice temperature, together with some fudge factors for crystal fabric or large-scale damage, so we know an approximate range of values that it could take.\n",
    "Nonetheless, we don't have large-scale measurements of the fluidity coefficient from remote sensing like we do for ice velocity and thickness.\n",
    "\n",
    "Instead, we can try to come up with a value of $A$ that gives a velocity field closest to what we observed.\n",
    "This idea can be turned into a constrained optimization problem.\n",
    "The quantity we wish to optimize is the misfit between the computed velocity $u$ and the observed velocity $u^o$:\n",
    "\n",
    "$$E(u) = \\frac{1}{2}\\int_\\Omega\\left(\\frac{u - u^o}{\\sigma}\\right)^2dx,$$\n",
    "\n",
    "where $\\sigma$ are the standard deviations of the measurements.\n",
    "\n",
    "One constraint is that the fluidity field has to be positive.\n",
    "Inequality constraints can require substantially more sophisticated numerical methods.\n",
    "To avoid this problem, we'll cheat our way out by reparameterizing $A$ in terms of a new variable $\\theta$:\n",
    "\n",
    "$$A = A_0e^\\theta.$$\n",
    "\n",
    "No matter the value of $\\theta$, $A$ is always positive.\n",
    "To make this change, we'll give the `IceShelf` object our own custom-made function for calculating the viscous part of the action functional, just like we did for the friction in the last demo.\n",
    "\n",
    "In addition to minimizing the misfit, we also want to have a relatively smooth value of the parameter field $\\theta$.\n",
    "The regularization functional $R$ is included to penalize oscillations of size $\\Theta$ over a given length scale $L$:\n",
    "\n",
    "$$R(\\theta) = \\frac{L^2}{2\\Theta^2}\\int_\\Omega|\\nabla \\theta|^2dx.$$\n",
    "\n",
    "Finally, let $F(u, \\theta)$ be the weak form of the shallow shelf equations, again using the new parameter $\\theta$ instead of the fluidity $A$.\n",
    "The physics constraint for our problem is that $F(u, \\theta) = 0$.\n",
    "We can enforce this constraint by introducing the Lagrange multiplier $\\lambda$, in which case the combined objective functional is\n",
    "\n",
    "$$J(u, \\theta; \\lambda) = E(u) + R(\\theta) + \\langle F(u, \\theta), \\lambda\\rangle.$$\n",
    "\n",
    "We can calculate the derivative of this functional with respect to $\\theta$ by using the *adjoint method*.\n",
    "We can then use a descent method to iterate towards a critical point, which is hopefully close to the true value of the fluidity coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input data\n",
    "\n",
    "The input data are the same as from the previous demo of the Larsen Ice Shelf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geojson\n",
    "import icepack\n",
    "\n",
    "outline_filename = icepack.datasets.fetch_larsen_outline()\n",
    "with open(outline_filename, 'r') as outline_file:\n",
    "    outline = geojson.load(outline_file)\n",
    "    \n",
    "geometry = icepack.meshing.collection_to_geo(outline)\n",
    "with open('larsen.geo', 'w') as geo_file:\n",
    "    geo_file.write(geometry.get_code())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gmsh -2 -format msh2 -v 2 -o larsen.msh larsen.geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import firedrake\n",
    "mesh = firedrake.Mesh('larsen.msh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "import icepack.plot\n",
    "\n",
    "features = [feature['geometry'] for feature in outline['features']]\n",
    "xmin, ymin, xmax, ymax = np.inf, np.inf, -np.inf, -np.inf\n",
    "δ = 50e3\n",
    "for feature in outline['features']:\n",
    "    for line_string in feature['geometry']['coordinates']:\n",
    "        xs = np.array(line_string)\n",
    "        x, y = xs[:, 0], xs[:, 1]\n",
    "        xmin, ymin = min(xmin, x.min() - δ), min(ymin, y.min() - δ)\n",
    "        xmax, ymax = max(xmax, x.max() + δ), max(ymax, y.max() + δ)\n",
    "        \n",
    "image_filename = icepack.datasets.fetch_mosaic_of_antarctica()\n",
    "with rasterio.open(image_filename, 'r') as image_file:\n",
    "    height, width = image_file.height, image_file.width\n",
    "    transform = image_file.transform\n",
    "    window = rasterio.windows.from_bounds(\n",
    "        left=xmin, bottom=ymin, right=xmax, top=ymax,\n",
    "        width=width, height=height, transform=transform\n",
    "    )\n",
    "    image = image_file.read(indexes=1, window=window, masked=True)\n",
    "\n",
    "def subplots(*args, **kwargs):\n",
    "    fig, axes = icepack.plot.subplots(*args, **kwargs)\n",
    "    xmin, ymin, xmax, ymax = rasterio.windows.bounds(window, transform)\n",
    "    kw = {\n",
    "        'extent': (xmin, xmax, ymin, ymax),\n",
    "        'cmap': 'Greys_r',\n",
    "        'vmin': 12e3,\n",
    "        'vmax':16.38e3\n",
    "    }\n",
    "    try:\n",
    "        axes.imshow(image, **kw)\n",
    "    except AttributeError:\n",
    "        for ax in axes:\n",
    "            ax.imshow(image, **kw)\n",
    "    \n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = subplots()\n",
    "axes.set_xlabel('meters')\n",
    "kwargs = {\n",
    "    'interior_kw': {'linewidth': .25},\n",
    "    'boundary_kw': {'linewidth': 2}\n",
    "}\n",
    "icepack.plot.triplot(mesh, axes=axes, **kwargs)\n",
    "axes.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in the 2nd demo, we'll apply the smoothing filter to the thickness, which is necessary to get a reasonable driving stress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from firedrake import inner, grad, dx\n",
    "\n",
    "thickness_filename = icepack.datasets.fetch_bedmachine_antarctica()\n",
    "thickness = rasterio.open(f'netcdf:{thickness_filename}:thickness', 'r')\n",
    "\n",
    "Q = firedrake.FunctionSpace(mesh, family='CG', degree=2)\n",
    "h0 = icepack.interpolate(thickness, Q)\n",
    "\n",
    "h = h0.copy(deepcopy=True)\n",
    "α = firedrake.Constant(2e3)\n",
    "J = 0.5 * ((h - h0)**2 + α**2 * inner(grad(h), grad(h))) * dx\n",
    "F = firedrake.derivative(J, h)\n",
    "firedrake.solve(F == 0, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the velocities themselves, we also need the estimates of the velocity measurement errors.\n",
    "The fidelity of the measurements tells us how good a fit to the data we should expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_filename = icepack.datasets.fetch_measures_antarctica()\n",
    "vx = rasterio.open(f'netcdf:{velocity_filename}:VX', 'r')\n",
    "vy = rasterio.open(f'netcdf:{velocity_filename}:VY', 'r')\n",
    "stdx = rasterio.open(f'netcdf:{velocity_filename}:ERRX', 'r')\n",
    "stdy = rasterio.open(f'netcdf:{velocity_filename}:ERRY', 'r')\n",
    "\n",
    "V = firedrake.VectorFunctionSpace(mesh, family='CG', degree=2)\n",
    "u_obs = icepack.interpolate((vx, vy), V)\n",
    "σx = icepack.interpolate(stdx, Q)\n",
    "σy = icepack.interpolate(stdy, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll plot the velocity errors.\n",
    "You can see from the stripey pattern that they depend on the particular swath from the observational platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "σ = firedrake.interpolate(firedrake.sqrt(σx**2 + σy**2), Q)\n",
    "fig, axes = subplots()\n",
    "colors = icepack.plot.tripcolor(σ, vmin=0, axes=axes)\n",
    "fig.colorbar(colors);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make an initial guess for the fluidity parameter.\n",
    "In this case, we'll use the same value as in the second demo -- a constant fluidity assuming a temperature of -13C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = firedrake.Constant(260)\n",
    "A0 = icepack.rate_factor(T)\n",
    "\n",
    "def viscosity(**kwargs):\n",
    "    u = kwargs['velocity']\n",
    "    h = kwargs['thickness']\n",
    "    θ = kwargs['log_fluidity']\n",
    "    \n",
    "    A = A0 * firedrake.exp(θ)\n",
    "    return icepack.models.viscosity.viscosity_depth_averaged(\n",
    "        velocity=u, thickness=h, fluidity=A\n",
    "    )\n",
    "\n",
    "θ = firedrake.Function(Q)\n",
    "\n",
    "model = icepack.models.IceShelf(viscosity=viscosity)\n",
    "opts = {'dirichlet_ids': [2, 4, 5, 6, 7, 8, 9]}\n",
    "solver = icepack.solvers.FlowSolver(model, **opts)\n",
    "\n",
    "u = solver.diagnostic_solve(\n",
    "    velocity=u_obs, \n",
    "    thickness=h,\n",
    "    log_fluidity=θ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the observed ice velocity and the computed of the ice velocity starting from our assumption that the fluidity is constant in space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = subplots(ncols=2, sharex=True, sharey=True)\n",
    "for ax in axes:\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "kwargs = {'precision': 1000, 'density': 2500, 'vmin': 0, 'vmax': 750}\n",
    "axes[0].set_title('Computed')\n",
    "axes[1].set_title('Observed')\n",
    "icepack.plot.streamplot(u, axes=axes[0], **kwargs)\n",
    "icepack.plot.streamplot(u_obs, axes=axes[1], **kwargs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few obvious missing features in our initial computed solution.\n",
    "For example, in this data, there's still a rift emanating from the Gipps Ice Rise.\n",
    "(In 2017, that rift finished propagating all the way across the terminus and broke off [Iceberg A-68](https://en.wikipedia.org/wiki/Iceberg_A-68).)\n",
    "Our initial computed velocity is smooth, but the observed velocity has a kink going across the rift.\n",
    "The objective of the exercise that follows is to compute a fluidity field that will reproduce features like the kink in the velocity pattern that emerges as a result of features like rifts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring the fluidity\n",
    "\n",
    "There are four parts that go into an inverse problem:\n",
    "\n",
    "* a physics model\n",
    "* an initial guess for the parameter and state\n",
    "* an error metric\n",
    "* a smoothness metric\n",
    "\n",
    "We already have the physics model and some initial guesses.\n",
    "The next step is to write a pair of Python functions that will create the model-data misfit functional and the regularization functional.\n",
    "We'll pass these functions to the inverse problem when we create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import icepack.inverse\n",
    "\n",
    "def objective(u):\n",
    "    δu = u - u_obs\n",
    "    return 0.5 * ((δu[0] / σx)**2 + (δu[1] / σy)**2) * dx\n",
    "\n",
    "Θ = firedrake.Constant(1.)\n",
    "L = firedrake.Constant(7.5e3)\n",
    "def regularization(θ):\n",
    "    return 0.5 * (L / Θ)**2 * inner(grad(θ), grad(θ)) * dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the `InverseProblem` object.\n",
    "We've already mentioned several objects that the inverse problem needs -- the model, the initial guess, some functionals, etc.\n",
    "Additionally, it needs to know the name of the observed field and the parameter (the `state_name` and `parameter_name`) arguments, since these values are passed to the forward solver as keyword arguments.\n",
    "\n",
    "The inverse solver will take this information and create a flow solver under the hood for us, but to do so it might need more information than we've already supplied.\n",
    "First, the keyword argument `solver_kwargs` takes a dictionary of additional keyword arguments to pass when initializing the flow solver, such as Dirichlet IDs or tolerances.\n",
    "Second, any additional arguments to the diagnostic solve procedure are passed in the dictionary `diagnostic_solve_kwargs`.\n",
    "In our case, that consists of just the thickness field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_problem = icepack.inverse.InverseProblem(\n",
    "    model=model,\n",
    "    objective=objective,\n",
    "    regularization=regularization,\n",
    "    state_name='velocity',\n",
    "    state=u,\n",
    "    parameter_name='log_fluidity',\n",
    "    parameter=θ,\n",
    "    solver_kwargs=opts,\n",
    "    diagnostic_solve_kwargs={'thickness': h}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've created the problem, we then create a solver object that will iteratively search for a good value of the parameters.\n",
    "The inverse solver has lots of moving parts, all of which are wrapped in a class that inherits from `icepack.inverse.InverseSolver`.\n",
    "In our case, we'll be using the Gauss-Newton method, which is implemented in `GaussNewtonSolver`.\n",
    "Using this class should save you from worrying about too many low-level details, but still provide a good amount of flexibility and transparency.\n",
    "\n",
    "As a convenience, the inverse solver can take in a function that it will call at the end of every iteration.\n",
    "For this demonstration, we'll have it print out the values of the misfit and regularization functionals and the expected decrease in the sum of those two functionals.\n",
    "The expected decrease in $J$ can be calculated as\n",
    "\n",
    "$$\\Delta = \\langle dJ, q\\rangle,$$\n",
    "\n",
    "where $dJ$ is the gradient of $J$ and $q$ is the search direction.\n",
    "The expected decrease should be negative on every step -- after all, we're trying to find a minimizer of $J$.\n",
    "If the expected decrease becomes positive, that suggests that we've found a local minimum and no more improvement is possible.\n",
    "This is an important number to watch because it can tell you whether or not it's worthwhile to take more iterations of the solver.\n",
    "\n",
    "We've also passed some extra arguments to `assemble` to specify the quadrature rule for calculating the expected decrease.\n",
    "The assembly routine will default to being very conservative and use a much more accurate quadrature rule than we really need and throw a warning about this behavior.\n",
    "This step isn't strictly necessary, it's just good practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdegree = model.quadrature_degree(\n",
    "    velocity=u, thickness=h, log_fluidity=θ\n",
    ")\n",
    "params = {'quadrature_degree': qdegree}\n",
    "\n",
    "area = firedrake.assemble(firedrake.Constant(1) * dx(mesh))\n",
    "def callback(inverse_solver):\n",
    "    dJ = inverse_solver.gradient\n",
    "    q = inverse_solver.search_direction\n",
    "    dJ_dq = firedrake.action(dJ, q)\n",
    "    Δ = firedrake.assemble(dJ_dq, form_compiler_parameters=params)\n",
    "\n",
    "    E = firedrake.assemble(inverse_solver.objective)\n",
    "    R = firedrake.assemble(inverse_solver.regularization)\n",
    "    print(f'{E / area:g}, {R / area:g}, {Δ / area:g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also, say, make a plot of the state and parameter guess at every iteration to make a movie of how the algorithm progresses.\n",
    "In programming parlance, a function like this is referred to as a *callback*.\n",
    "\n",
    "To create the solver object, we only need to give it a problem and optionally the callback function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_solver = icepack.inverse.GaussNewtonSolver(\n",
    "    inverse_problem, callback, search_max_iterations=300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a general principle, it's a good idea to separate out the *specification* of a problem, in this case represented by the `InverseProblem` class, from the method used to solve the problem, represented by the `GaussNewtonSolver` class.\n",
    "We've already seen this pattern before in how flow models (like `IceShelf`) are distinct from the objects that actually calculate the solutions (`FlowSolver`).\n",
    "\n",
    "Before setting the solver loose, let's look at the initial search direction.\n",
    "The search direction is zero throughout most of the ice shelf.\n",
    "It has strong positive anomalies in the vicinity of the sharp rifts on the grid-east side of the domain.\n",
    "In other words, starting from a spatially constant initial guess, the fastest way to reduce the model-data misfit would be to make the shelf more fluid in the vicinity of these rifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = subplots()\n",
    "ϕ = inverse_solver.search_direction\n",
    "colors = icepack.plot.tripcolor(\n",
    "    ϕ, vmin=-8, vmax=+8, cmap='RdBu_r', axes=axes\n",
    ")\n",
    "fig.colorbar(colors);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search direction is obtained by multiplying the inverse of the Gauss-Newton matrix $H$ by the gradient $dJ$ of the objective function.\n",
    "The Gauss-Newton matrix is dense, so we don't actually build the matrix directly.\n",
    "Instead, the solver contains a procedure to multiply a vector by $H$, which is all that's necessary for using iterative methods to solve linear systems.\n",
    "Computing the search direction like this is time-consuming, but results in far fewer iterations, so it's a net win.\n",
    "\n",
    "The solve method takes in a relative convergence tolerance, an absolute tolerance, and a maximum number of iterations, and it returns the total number of iterations necessary to achieve the given tolerances.\n",
    "In our case, we'll stop once either (1) the relative decrease in the objective function from one iteration to the next is less than 1/200 or (2) the expected relative decrease in the objective is less than 1/1,000,000.\n",
    "The relevant tolerances for these are respectively `rtol` and `etol`.\n",
    "There's also an absolute stopping tolerance `atol`, which we won't use this time.\n",
    "Absolute stopping tolerances are good for certain statistical problems where you know a priori how good of a fit to the data to expect.\n",
    "\n",
    "The algorithm takes about 30-45 minutes to run.\n",
    "Now would be the time to put on a fresh pot of coffee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = inverse_solver.solve(\n",
    "    rtol=5e-3,\n",
    "    etol=1e-6,\n",
    "    atol=0.0,\n",
    "    max_iterations=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm converges in just a few iterations because of how good a search direction we get from using the Gauss-Newton approximation.\n",
    "It's also worth observing how quickly the magnitude of the expected decrease in the objective functional gets reduced on every iteration -- usually by at least a factor of four every time.\n",
    "Other methods like gradient descent take many more iterations to reach the same agreement with the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Now that we're done, we'll want to do some post-processing and analysis on the fluidity parameter that we inferred.\n",
    "The inverse problem object stores the parameter we're inferring and the observed field as the properties `parameter` and `state` respectively.\n",
    "The names are intentionally not specific to just ice shelves.\n",
    "For other problems, we might instead be inferring a friction coefficient rather than a fluidity, or we might be observing the thickness instead of the velocity.\n",
    "You can see all the publicly visible properties by typing `help(inverse_problem)`.\n",
    "\n",
    "First, let's plot the parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "θ = inverse_solver.parameter\n",
    "fig, axes = subplots()\n",
    "colors = icepack.plot.tripcolor(\n",
    "    θ, vmin=-5, vmax=+5, axes=axes\n",
    ")\n",
    "fig.colorbar(colors);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fluidity is much higher around areas of heavy crevassing, such as the rift opening from the Gipps Ice Rise and the area flowing into it.\n",
    "Additionally, several areas downstream of the inlets have much higher fluidity, and these might indicate the formation of marine ice.\n",
    "\n",
    "The fluidity is substantially lower around the upper left edge of the ice shelf.\n",
    "Some of the ice might actually be grounded here, in which case the resulting basal drag would tend to reduce the extensional strain of the glacier.\n",
    "However, since the only tunable variable for explaining the observed velocities is the fluidity, the algorithm will erroneously converge on whatever value of the fluidity can reproduce the observed values.\n",
    "In this case, the result is a very low value of $A$, but for other problems the bias can go in the other direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how well the parameters fit the data, let's look at the weighted difference between the computed and observed velocities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = inverse_solver.state\n",
    "fig, axes = subplots()\n",
    "δu = firedrake.interpolate((u - u_obs)**2/(2*σ**2), Q)\n",
    "colors = icepack.plot.tripcolor(\n",
    "    δu, vmin=0, vmax=50, cmap='Reds', axes=axes\n",
    ")\n",
    "fig.colorbar(colors);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computed ice velocity is mostly similar to observations aside from a few blips.\n",
    "The most widespread departures from observations occur around the rifts that flow into the Gipps Ice Rise.\n",
    "We regularized the problem by looking only for smooth values of the fluidity parameter.\n",
    "As a consequence, we won't be able to see sharp changes that might result from features like crevasses or rifts.\n",
    "We might instead try to use the total variation functional\n",
    "\n",
    "$$R(\\theta) = L\\int_\\Omega|\\nabla\\theta|dx$$\n",
    "\n",
    "if we were interested in features like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's try and see how much the inferred parameter departed from our naive initial guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(icepack.norm(inverse_solver.parameter) / np.sqrt(area))\n",
    "print(firedrake.assemble(inverse_solver.objective) / area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model-data misfit has been reduced by two orders of mangitude through the optimization procedure, and our final approximation departs quite substantially from the initial guess.\n",
    "This suggests that data assimilation does give a substantial benefit over an ad-hoc approach like picking a sensible constant value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this demo, we've shown how to back out the fluidity of an ice shelf from remote sensing observations.\n",
    "We could then use this value, together with a description of how the fluidity evolves, to initialize a prognostic model of the ice shelf.\n",
    "For example, we might assume that the fluidity is a function of ice temperature and damage.\n",
    "The evolution equations for these fields are fairly simple hyperbolic PDE for which we can write solvers using firedrake.\n",
    "\n",
    "The value of the fluidity that we obtained is not at all spatially homogeneous.\n",
    "Unless we were very clever, we probably couldn't have come up with some way to parameterize it to get a reasonable guess.\n",
    "\n",
    "We would expect from statistical estimation theory that the value of the misfit functional divided by the shelf area will be around 1.\n",
    "(A sum of squares of normal random variables has a $\\chi^2$ distribution, which has mean 1, there are two components of the velocity vector, and we divide by 2 at the end.)\n",
    "The misfit we obtained once the algorithm has converged is much larger than 1.\n",
    "Why might this happen?\n",
    "\n",
    "1. We made a poor choice of regularization parameter; the solution is too smooth to fit the data.\n",
    "2. The regularization parameter is fine, but the error estimates $\\sigma$ are wrong.\n",
    "3. The standard deviations $\\sigma$ of the error estimates are correct, but the error distribution is non-normal and has heavier tails.\n",
    "4. We don't have a good way to also account for thickness errors, which are substantial.\n",
    "5. The ice shelf is actually grounded on some isolated pinning points or ice rises and we didn't add any basal drag.\n",
    "6. The model physics don't adequately account for the effects of rifts and crevasses.\n",
    "7. I implemented the numerical optimization algorithm incorrectly.\n",
    "\n",
    "Failure modes 1 happens because we don't have the right prior distribution, while modes 2, 3, and 4 occur because we don't have the correct observational likelihood.\n",
    "Modes 5 and 6 are more insidious types of failure.\n",
    "In this case, the physics model doesn't actually describe the true behavior of the system.\n",
    "This might mean that the model cannot reproduce the observations with *any* value of the input data.\n",
    "That kind of failure is difficult to miss.\n",
    "But some wrong physics models -- those that are [controllable](https://en.wikipedia.org/wiki/Controllability) as a function of the input parameters -- might still be able to reproduce the observations.\n",
    "The catch is that they will \"explain\" the data however they can, even if the reasons are wrong!\n",
    "Diagnosing this type of failure is arguably the most difficult.\n",
    "Last but not least is human error in implementing the optimization algorithms.\n",
    "These kinds of failures should be caught through testing on synthetic problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firedrake",
   "language": "python",
   "name": "firedrake"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
